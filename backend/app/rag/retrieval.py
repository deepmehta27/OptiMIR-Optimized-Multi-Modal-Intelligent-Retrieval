import json
import time
from typing import List, Literal, AsyncGenerator
from pydantic import BaseModel
from openai import AsyncOpenAI
import anthropic
from .config import OPENAI_API_KEY, ANTHROPIC_API_KEY
from .ingest import get_or_create_collection

# --- Schemas ---
class QueryRequest(BaseModel):
    question: str
    model: Literal["gpt4o", "claude"] = "gpt4o"

class RetrievedChunk(BaseModel):
    text: str
    source: str
    score: float
    type: str | None = None
    page: int | None = None

class RAGResponse(BaseModel):
    answer: str
    model: str
    chunks: List[RetrievedChunk]

# Retrieval + Prompt

async def retrieve_chunks(query: str, k: int = 4) -> List[RetrievedChunk]:
    """Sync Chroma query wrapped in async function for consistency."""
    collection = get_or_create_collection()
    results = collection.query(query_texts=[query], n_results=k)
    chunks: List[RetrievedChunk] = []
    for doc_id, doc, meta, dist in zip(results["ids"][0],results["documents"][0],results["metadatas"][0],results["distances"][0],
    ):
        chunks.append(
            RetrievedChunk(
                text=doc,
                source=meta.get("source", doc_id),
                score=float(dist),
                type=meta.get("type"),
                page=meta.get("page"),
            )
        )
    return chunks

def build_prompt(query: str, chunks: List[RetrievedChunk]) -> str:
    # Format chunks with clear ID and Source labels
    context_entries = []
    for i, c in enumerate(chunks):
        context_entries.append(f"<document id='{i+1}'>\nSource: {c.source}\nContent: {c.text}\n</document>")
    context_str = "\n".join(context_entries)

    return f"""
You are a highly precise Research Assistant. Your goal is to answer questions using ONLY the provided context.

### INSTRUCTIONS:
1. **Grounding:** Base your entire response solely on the information inside the <context> tags.
2. **Strict Refusal:** If the context does NOT contain the answer, state: "I am sorry, but the provided documents do not contain information to answer this question." Do NOT use your own knowledge.
3. **Citations:** Every factual claim must be followed by a citation in brackets, e.g., [1] or [1, 3].
4. **Irrelevance Filter:** Ignore any information in the context that is not directly related to the question.
5. **No Hallucination:** Do not infer details or "read between the lines" if the data is missing.

<context>
{context_str}
</context>

<question>
{query}
</question>

Answer:""".strip()

# NON-STREAMING LLM CALLS (for /query)

async def answer_with_openai(prompt: str) -> str:
    start_time = time.time()
    client = AsyncOpenAI(api_key=OPENAI_API_KEY)

    resp = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=1024,
        temperature=0.1,
    )

    print(f"--- [LATENCY] GPT-4o-mini: {time.time() - start_time:.2f}s ---")
    return resp.choices[0].message.content or ""

async def answer_with_claude(prompt: str) -> str:
    start_time = time.time()
    client = anthropic.AsyncAnthropic(api_key=ANTHROPIC_API_KEY)

    resp = await client.messages.create(
        model="claude-haiku-4-5",
        max_tokens=1024,
        temperature=0.1,
        messages=[{"role": "user", "content": prompt}],
    )

    print(f"--- [LATENCY] Claude-Haiku-4.5: {time.time() - start_time:.2f}s ---")
    return "".join(block.text for block in resp.content if block.type == "text")

async def rag_answer(
    query: str,
    model: Literal["gpt4o", "claude"] = "gpt4o",
) -> RAGResponse:
    chunks = await retrieve_chunks(query)
    prompt = build_prompt(query, chunks)

    print(f"\n--- [LOG] Routing to: {model.upper()} ---")

    if model == "claude":
        answer = await answer_with_claude(prompt)
        model_name = "claude-haiku-4.5"
    else:
        answer = await answer_with_openai(prompt)
        model_name = "gpt-4o-mini"

    print(f"--- [LOG] Answer generated by: {model_name} ---\n")

    return RAGResponse(answer=answer, model=model_name, chunks=chunks)

# STREAMING LLM CALLS (for /query/stream)

async def stream_openai(prompt: str) -> AsyncGenerator[str, None]:
    """Yield SSE events with tokens from GPT-4o-mini."""
    start_time = time.time()
    client = AsyncOpenAI(api_key=OPENAI_API_KEY)

    stream = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=1024,
        temperature=0.1,
        stream=True,
    )

    first_token = True
    full_answer: List[str] = []

    async for chunk in stream:
        delta = chunk.choices[0].delta
        token = delta.content or ""
        if not token:
            continue

        full_answer.append(token)

        if first_token:
            print(f"--- [TTFT] OpenAI: {time.time() - start_time:.2f}s ---")
            first_token = False

        yield f"data: {json.dumps({'type': 'token', 'token': token})}\n\n"

    print(f"--- [LATENCY] OpenAI Stream Total: {time.time() - start_time:.2f}s ---")
    # full answer available as "".join(full_answer) if needed


async def stream_claude(prompt: str) -> AsyncGenerator[str, None]:
    """Yield SSE events with tokens from Claude Haiku 4.5."""
    start_time = time.time()
    client = anthropic.AsyncAnthropic(api_key=ANTHROPIC_API_KEY)

    first_token = True
    full_answer: List[str] = []

    async with client.messages.stream(
        model="claude-haiku-4-5",
        max_tokens=1024,
        temperature=0.1,
        messages=[{"role": "user", "content": prompt}],
    ) as stream:
        async for event in stream:
            if event.type == "content_block_delta" and event.delta.type == "text_delta":
                token = event.delta.text
                if not token:
                    continue

                full_answer.append(token)

                if first_token:
                    print(f"--- [TTFT] Claude: {time.time() - start_time:.2f}s ---")
                    first_token = False

                yield f"data: {json.dumps({'type': 'token', 'token': token})}\n\n"

    print(f"--- [LATENCY] Claude Stream Total: {time.time() - start_time:.2f}s ---")
    # full answer available as "".join(full_answer) if needed


async def stream_rag_answer(
    query: str,
    model: Literal["gpt4o", "claude"] = "gpt4o",
) -> AsyncGenerator[str, None]:
    """Orchestrate retrieval + prompt + model streaming, yielding SSE events."""
    chunks = await retrieve_chunks(query)
    prompt = build_prompt(query, chunks)

    # 1) Send metadata event first (model + chunks)
    meta = {
        "type": "meta",
        "model": "claude-haiku-4.5" if model == "claude" else "gpt-4o-mini",
        "chunks": [c.model_dump() for c in chunks],
    }
    yield f"data: {json.dumps(meta)}\n\n"

    # 2) Stream tokens
    if model == "claude":
        async for event in stream_claude(prompt):
            yield event
    else:
        async for event in stream_openai(prompt):
            yield event

    # 3) Final done marker
    yield "data: [DONE]\n\n"